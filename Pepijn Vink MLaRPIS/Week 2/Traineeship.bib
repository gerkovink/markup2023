@article{andersson1998a,
  title = {Normal {{Linear Regression Models With Recursive Graphical Markov Structure}}},
  author = {Andersson, Steen A and Perlman, Michael D},
  year = {1998},
  month = aug,
  journal = {Journal of Multivariate Analysis},
  volume = {66},
  number = {2},
  pages = {133--187},
  issn = {0047259X},
  doi = {10.1006/jmva.1998.1745},
  urldate = {2023-11-23},
  langid = {english},
  keywords = {to read},
  file = {/Users/pepijnvink/Zotero/storage/TPKY9PDU/Andersson and Perlman - 1998 - Normal Linear Regression Models With Recursive Gra.pdf}
}

@article{aragam2019,
  title = {Learning {{Large-Scale Bayesian Networks}} with the Sparsebn {{Package}}},
  author = {Aragam, Bryon and Gu, Jiaying and Zhou, Qing},
  year = {2019},
  journal = {Journal of Statistical Software},
  volume = {91},
  number = {11},
  eprint = {1703.04025},
  primaryclass = {cs, stat},
  issn = {1548-7660},
  doi = {10.18637/jss.v091.i11},
  urldate = {2023-11-22},
  abstract = {Learning graphical models from data is an important problem with wide applications, ranging from genomics to the social sciences. Nowadays datasets often have upwards of thousands---sometimes tens or hundreds of thousands---of variables and far fewer samples. To meet this challenge, we have developed a new R package called sparsebn for learning the structure of large, sparse graphical models with a focus on Bayesian networks. While there are many existing software packages for this task, this package focuses on the unique setting of learning large networks from high-dimensional data, possibly with interventions. As such, the methods provided place a premium on scalability and consistency in a high-dimensional setting. Furthermore, in the presence of interventions, the methods implemented here achieve the goal of learning a causal network from data. Additionally, the sparsebn package is fully compatible with existing software packages for network analysis.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology,to read},
  file = {/Users/pepijnvink/Zotero/storage/K6S2ZQRD/Aragam et al. - 2019 - Learning Large-Scale Bayesian Networks with the sp.pdf;/Users/pepijnvink/Zotero/storage/R9Q8UE22/1703.html}
}

@misc{ben-david2015,
  title = {High Dimensional {{Bayesian}} Inference for {{Gaussian}} Directed Acyclic Graph Models},
  author = {{Ben-David}, Emanuel and Li, Tianxi and Massam, Helene and Rajaratnam, Bala},
  year = {2015},
  month = mar,
  number = {arXiv:1109.4371},
  eprint = {1109.4371},
  publisher = {{arXiv}},
  urldate = {2023-10-23},
  abstract = {In this paper, we consider Gaussian models Markov with respect to an arbitrary DAG. We first construct a family of conjugate priors for the Cholesky parametrization of the covariance matrix of such models. This family has as many shape parameters as the DAG has vertices, and naturally extends the work of Geiger and Heckerman [8]. From these distributions, we derive prior distributions for the covariance and precision parameters of the Gaussian DAG Markov models. Our works thus extends the work of Dawid and Lauritzen [5] and Letac and Massam [16] for Gaussian models Markov with respect to a decomposable graph to arbitrary DAGs. For this reason, we call our distributions DAG-Wishart distributions. An advantage of these distributions is that they possess strong hyper Markov properties and thus allow for explicit estimation of the covariance and precision parameters, regardless of the dimension of the problem. They also allow us to develop methodology for model selection and covariance estimation in the space of DAG-Markov models. We demonstrate via several numerical examples that the proposed method scales well to high-dimensions.},
  archiveprefix = {arxiv},
  keywords = {{62-09, 62E10, 62J05},Mathematics - Statistics Theory,read,Statistics - Other Statistics},
  file = {/Users/pepijnvink/Zotero/storage/UXXY62AX/Ben-David et al. - 2015 - High dimensional Bayesian inference for Gaussian d.pdf;/Users/pepijnvink/Zotero/storage/M83H3WHA/1109.html}
}

@article{cao2019,
  title = {Posterior Graph Selection and Estimation Consistency for High-Dimensional {{Bayesian DAG}} Models},
  author = {Cao, Xuan and Khare, Kshitij and Ghosh, Malay},
  year = {2019},
  month = feb,
  journal = {The Annals of Statistics},
  volume = {47},
  number = {1},
  issn = {0090-5364},
  doi = {10.1214/18-AOS1689},
  urldate = {2023-09-14},
  langid = {english},
  keywords = {read},
  file = {/Users/pepijnvink/Zotero/storage/J8M9ATSR/Cao et al. - 2019 - Posterior graph selection and estimation consisten.pdf}
}

@article{castelletti2018,
  title = {Learning {{Markov Equivalence Classes}} of {{Directed Acyclic Graphs}}: {{An Objective Bayes Approach}}},
  shorttitle = {Learning {{Markov Equivalence Classes}} of {{Directed Acyclic Graphs}}},
  author = {Castelletti, Federico and Consonni, Guido and Della Vedova, Marco L. and Peluso, Stefano},
  year = {2018},
  month = dec,
  journal = {Bayesian Analysis},
  volume = {13},
  number = {4},
  issn = {1936-0975},
  doi = {10.1214/18-BA1101},
  urldate = {2023-09-14},
  abstract = {A Markov equivalence class contains all the Directed Acyclic Graphs (DAGs) encoding the same conditional independencies, and is represented by a Completed Partially Directed Acyclic Graph (CPDAG), also named Essential Graph (EG). We approach the problem of model selection among noncausal sparse Gaussian DAGs by directly scoring EGs, using an objective Bayes method. Specifically, we construct objective priors for model selection based on the Fractional Bayes Factor, leading to a closed form expression for the marginal likelihood of an EG. Next we propose a Markov Chain Monte Carlo (MCMC) strategy to explore the space of EGs using sparsity constraints, and illustrate the performance of our method on simulation studies, as well as on a real dataset. Our method provides a coherent quantification of inferential uncertainty, requires minimal prior specification, and shows to be competitive in learning the structure of the data-generating EG when compared to alternative state-of-the-art algorithms.},
  langid = {english},
  keywords = {read},
  file = {/Users/pepijnvink/Zotero/storage/KHBIIB6M/Castelletti et al. - 2018 - Learning Markov Equivalence Classes of Directed Ac.pdf}
}

@misc{castelletti2023,
  title = {Joint Structure Learning and Causal Effect Estimation for Categorical Graphical Models},
  author = {Castelletti, Federico and Consonni, Guido and Della Vedova, Marco Luigi},
  year = {2023},
  month = jun,
  number = {arXiv:2306.16068},
  eprint = {2306.16068},
  primaryclass = {stat},
  publisher = {{arXiv}},
  urldate = {2023-09-14},
  abstract = {We consider a a collection of categorical random variables. Of special interest is the causal effect on an outcome variable following an intervention on another variable. Conditionally on a Directed Acyclic Graph (DAG), we assume that the joint law of the random variables can be factorized according to the DAG, where each term is a categorical distribution for the node-variable given a configuration of its parents. The graph is equipped with a causal interpretation through the notion of interventional distribution and the allied ``do-calculus''. From a modeling perspective, the likelihood is decomposed into a product over nodes and parents of DAG-parameters, on which a suitably specified collection of Dirichlet priors is assigned. The overall joint distribution on the ensemble of DAG-parameters is then constructed using global and local independence. We account for DAG-model uncertainty and propose a reversible jump Markov Chain Monte Carlo (MCMC) algorithm which targets the joint posterior over DAGs and DAG-parameters; from the output we are able to recover a full posterior distribution of any causal effect coefficient of interest, possibly summarized by a Bayesian Model Averaging (BMA) point estimate. We validate our method through extensive simulation studies, wherein comparisons with alternative state-ofthe-art procedures reveal an outperformance in terms of estimation accuracy. Finally, we analyze a dataset relative to a study on depression and anxiety in undergraduate students.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Statistics - Computation,Statistics - Methodology,to read},
  file = {/Users/pepijnvink/Zotero/storage/EB5N6U4F/Castelletti et al. - 2023 - Joint structure learning and causal effect estimat.pdf}
}

@article{consonni2017,
  title = {Objective {{Bayes Covariate}}-{{Adjusted Sparse Graphical Model Selection}}},
  author = {Consonni, Guido and La Rocca, Luca and Peluso, Stefano},
  year = {2017},
  journal = {Scandinavian Journal of Statistics},
  volume = {44},
  pages = {741--764},
  doi = {10.1111/sjos.12273},
  urldate = {2023-11-23},
  keywords = {to read},
  file = {/Users/pepijnvink/Zotero/storage/WH4PD8WK/Consonni - 2017 - Objective Bayes Covariate‚ÄêAdjusted Sparse Graphica.pdf;/Users/pepijnvink/Zotero/storage/K4NHBIYX/sjos.html}
}

@article{constantinou2020,
  title = {Learning {{Bayesian Networks}} with the {{Saiyan Algorithm}}},
  author = {Constantinou, Anthony C.},
  year = {2020},
  month = jun,
  journal = {ACM Transactions on Knowledge Discovery from Data},
  volume = {14},
  number = {4},
  pages = {44:1--44:21},
  issn = {1556-4681},
  doi = {10.1145/3385655},
  urldate = {2023-11-22},
  abstract = {Some structure learning algorithms have proven to be effective in reconstructing hypothetical Bayesian Network graphs from synthetic data. However, in their mission to maximise a scoring function, many become conservative and minimise edges discovered. While simplicity is desired, the output is often a graph that consists of multiple independent subgraphs that do not enable full propagation of evidence. While this is not a problem in theory, it can be a problem in practice. This article examines a novel unconventional associational heuristic called Saiyan, which returns a directed acyclic graph that enables full propagation of evidence. Associational heuristics are not expected to perform well relative to sophisticated constraint-based and score-based learning approaches. Moreover, forcing the algorithm to connect all data variables implies that the forced edges will not be correct at the rate of those identified unrestrictedly. Still, synthetic and real-world experiments suggest that such a heuristic can be competitive relative to some of the well-established constraint-based, score-based and hybrid learning algorithms.},
  keywords = {Bayesian networks,directed acyclic graphs,graphical models,structure learning,to read},
  file = {/Users/pepijnvink/Zotero/storage/9T7YZHIG/Constantinou - 2020 - Learning Bayesian Networks with the Saiyan Algorit.pdf}
}

@article{datta2017,
  title = {Graph\_sampler: A Simple Tool for Fully {{Bayesian}} Analyses of {{DAG-models}}},
  shorttitle = {Graph\_sampler},
  author = {Datta, Sagnik and Gayraud, Ghislaine and Leclerc, Eric and Bois, Frederic Y.},
  year = {2017},
  month = jun,
  journal = {Computational Statistics},
  volume = {32},
  number = {2},
  pages = {691--716},
  issn = {0943-4062, 1613-9658},
  doi = {10.1007/s00180-017-0719-1},
  urldate = {2023-09-14},
  langid = {english},
  keywords = {to read},
  file = {/Users/pepijnvink/Zotero/storage/F8ZDEUC6/Datta et al. - 2017 - Graph_sampler a simple tool for fully Bayesian an.pdf}
}

@book{geffner2022,
  title = {Probabilistic and {{Causal Inference}}: {{The Works}} of {{Judea Pearl}}},
  author = {Geffner, Hector and Dechter, Rina and Halpern, Joseph Y.},
  year = {2022},
  series = {{{ACM Books}}},
  number = {36},
  publisher = {{Association of Computing Machinery}},
  langid = {english},
  file = {/Users/pepijnvink/Zotero/storage/ZRNQMPC2/Probabilistic and Causal Inference The Works of J.pdf}
}

@article{geiger2002,
  title = {Parameter Priors for Directed Acyclic Graphical Models and the Characterization of Several Probability Distributions},
  author = {Geiger, Dan and Heckerman, David},
  year = {2002},
  month = oct,
  journal = {The Annals of Statistics},
  volume = {30},
  number = {5},
  issn = {0090-5364},
  doi = {10.1214/aos/1035844981},
  urldate = {2023-09-14},
  langid = {english},
  keywords = {read},
  file = {/Users/pepijnvink/Zotero/storage/NMKDJ3AY/Geiger and Heckerman - 2002 - Parameter priors for directed acyclic graphical mo.pdf}
}

@article{glymour2019,
  title = {Review of {{Causal Discovery Methods Based}} on {{Graphical Models}}},
  author = {Glymour, Clark and Zhang, Kun and Spirtes, Peter},
  year = {2019},
  journal = {Frontiers in Genetics},
  volume = {10},
  issn = {1664-8021},
  urldate = {2023-11-22},
  abstract = {A fundamental task in various disciplines of science, including biology, is to find underlying causal relations and make use of them. Causal relations can be seen if interventions are properly applied; however, in many cases they are difficult or even impossible to conduct. It is then necessary to discover causal relations by analyzing statistical properties of purely observational data, which is known as causal discovery or causal structure search. This paper aims to give a introduction to and a brief review of the computational methods for causal discovery that were developed in the past three decades, including constraint-based and score-based methods and those based on functional causal models, supplemented by some illustrations and applications.},
  keywords = {read},
  file = {/Users/pepijnvink/Zotero/storage/G3TSE8YT/Glymour et al. - 2019 - Review of Causal Discovery Methods Based on Graphi.pdf}
}

@article{goudie2016,
  title = {A {{Gibbs Sampler}} for {{Learning DAGs}}},
  author = {Goudie, Robert J B},
  year = {2016},
  journal = {Journal of Machine Learning Research},
  volume = {17},
  pages = {1--39},
  abstract = {We propose a Gibbs sampler for structure learning in directed acyclic graph (DAG) models. The standard Markov chain Monte Carlo algorithms used for learning DAGs are random-walk Metropolis-Hastings samplers. These samplers are guaranteed to converge asymptotically but often mix slowly when exploring the large graph spaces that arise in structure learning. In each step, the sampler we propose draws entire sets of parents for multiple nodes from the appropriate conditional distribution. This provides an efficient way to make large moves in graph space, permitting faster mixing whilst retaining asymptotic guarantees of convergence. The conditional distribution is related to variable selection with candidate parents playing the role of covariates or inputs. We empirically examine the performance of the sampler using several simulated and real data examples. The proposed method gives robust results in diverse settings, outperforming several existing Bayesian and frequentist methods. In addition, our empirical results shed some light on the relative merits of Bayesian and constraint-based methods for structure learning.},
  langid = {english},
  keywords = {to read},
  file = {/Users/pepijnvink/Zotero/storage/W8NZ3EDA/Goudie - A Gibbs Sampler for Learning DAGs.pdf}
}

@article{han2016,
  title = {Estimation of {{Directed Acyclic Graphs Through Two-Stage Adaptive Lasso}} for {{Gene Network Inference}}},
  author = {Han, Sung Won and Chen, Gong and Cheon, Myun-Seok and Zhong, Hua},
  year = {2016},
  month = jul,
  journal = {Journal of the American Statistical Association},
  volume = {111},
  number = {515},
  pages = {1004--1019},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.2016.1142880},
  urldate = {2023-11-22},
  abstract = {Graphical models are a popular approach to find dependence and conditional independence relationships between gene expressions. Directed acyclic graphs (DAGs) are a special class of directed graphical models, where all the edges are directed edges and contain no directed cycles. The DAGs are well known models for discovering causal relationships between genes in gene regulatory networks. However, estimating DAGs without assuming known ordering is challenging due to high dimensionality, the acyclic constraints, and the presence of equivalence class from observational data. To overcome these challenges, we propose a two-stage adaptive Lasso approach, called NS-DIST, which performs neighborhood selection (NS) in stage 1, and then estimates DAGs by the discrete improving search with Tabu (DIST) algorithm within the selected neighborhood. Simulation studies are presented to demonstrate the effectiveness of the method and its computational efficiency. Two real data examples are used to demonstrate the practical usage of our method for gene regulatory network inference. Supplementary materials for this article are available online.},
  pmid = {28239216},
  keywords = {Directed acyclic graphs,Lasso estimation,Neighborhood selection,Probabilistic graphical model,Structure equation model,to read},
  file = {/Users/pepijnvink/Zotero/storage/A3C9KDXG/Han et al. - 2016 - Estimation of Directed Acyclic Graphs Through Two-.pdf}
}

@article{he2013,
  title = {Reversible {{MCMC}} on {{Markov}} Equivalence Classes of Sparse Directed Acyclic Graphs},
  author = {He, Yangbo and Jia, Jinzhu and Yu, Bin},
  year = {2013},
  month = aug,
  journal = {The Annals of Statistics},
  volume = {41},
  number = {4},
  issn = {0090-5364},
  doi = {10.1214/13-AOS1125},
  urldate = {2023-10-04},
  langid = {english},
  keywords = {to read},
  file = {/Users/pepijnvink/Zotero/storage/XZL843GW/He et al. - 2013 - Reversible MCMC on Markov equivalence classes of s.pdf}
}

@article{heckerman1995,
  title = {Learning {{Bayesian}} Networks: {{The}} Combination of Knowledge and Statistical Data},
  shorttitle = {Learning {{Bayesian}} Networks},
  author = {Heckerman, David and Geiger, Dan and Chickering, David M.},
  year = {1995},
  month = sep,
  journal = {Machine Learning},
  volume = {20},
  number = {3},
  pages = {197--243},
  issn = {1573-0565},
  doi = {10.1007/BF00994016},
  urldate = {2024-01-11},
  abstract = {We describe a Bayesian approach for learning Bayesian networks from a combination of prior knowledge and statistical data. First and foremost, we develop a methodology for assessing informative priors needed for learning. Our approach is derived from a set of assumptions made previously as well as the assumption oflikelihood equivalence, which says that data should not help to discriminate network structures that represent the same assertions of conditional independence. We show that likelihood equivalence when combined with previously made assumptions implies that the user's priors for network parameters can be encoded in a single Bayesian network for the next case to be seen{\textemdash}aprior network{\textemdash}and a single measure of confidence for that network. Second, using these priors, we show how to compute the relative posterior probabilities of network structures given data. Third, we describe search methods for identifying network structures with high posterior probabilities. We describe polynomial algorithms for finding the highest-scoring network structures in the special case where every node has at mostk=1 parent. For the general case (k{$>$}1), which is NP-hard, we review heuristic search algorithms including local search, iterative local search, and simulated annealing. Finally, we describe a methodology for evaluating Bayesian-network learning algorithms, and apply this approach to a comparison of various approaches.},
  langid = {english},
  keywords = {Bayesian networks,Dirichlet,heuristic search,learning,likelihood equivalence,maximum branching},
  file = {/Users/pepijnvink/Zotero/storage/J5A8GYMV/Heckerman et al. - 1995 - Learning Bayesian networks The combination of kno.pdf}
}

@misc{heckerman2021,
  title = {Likelihoods and {{Parameter Priors}} for {{Bayesian Networks}}},
  author = {Heckerman, David and Geiger, Dan},
  year = {2021},
  month = jun,
  number = {arXiv:2105.06241},
  eprint = {2105.06241},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-11-22},
  abstract = {We develop simple methods for constructing likelihoods and parameter priors for learning about the parameters and structure of a Bayesian network. In particular, we introduce several assumptions that permit the construction of likelihoods and parameter priors for a large number of Bayesian-network structures from a small set of assessments. The most notable assumption is that of likelihood equivalence, which says that data can not help to discriminate network structures that encode the same assertions of conditional independence. We describe the constructions that follow from these assumptions, and also present a method for directly computing the marginal likelihood of a random sample with no missing observations. Also, we show how these assumptions lead to a general framework for characterizing parameter priors of multivariate distributions.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,G.3,I.2,Statistics - Machine Learning,to read},
  file = {/Users/pepijnvink/Zotero/storage/DH8TZVDR/Heckerman and Geiger - 2021 - Likelihoods and Parameter Priors for Bayesian Netw.pdf;/Users/pepijnvink/Zotero/storage/5SVKCUWW/2105.html}
}

@article{huang2013,
  title = {A {{Sparse Structure Learning Algorithm}} for {{Gaussian Bayesian Network Identification}} from {{High-Dimensional Data}}},
  author = {Huang, Shuai and Li, Jing and Ye, Jieping and Fleisher, Adam and Chen, Kewei and Wu, Teresa and Reiman, Eric and Alzheimer's Disease Neuroimaging Initiative, the},
  year = {2013},
  month = jun,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {35},
  number = {6},
  pages = {1328--1342},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2012.129},
  urldate = {2023-11-22},
  abstract = {Structure learning of Bayesian Networks (BNs) is an important topic in machine learning. Driven by modern applications in genetics and brain sciences, accurate and efficient learning of large-scale BN structures from high-dimensional data becomes a challenging problem. To tackle this challenge, we propose a Sparse Bayesian Network (SBN) structure learning algorithm that employs a novel formulation involving one L1-norm penalty term to impose sparsity and another penalty term to ensure that the learned BN is a Directed Acyclic Graph (DAG){\textemdash}a required property of BNs. Through both theoretical analysis and extensive experiments on 11 moderate and large benchmark networks with various sample sizes, we show that SBN leads to improved learning accuracy, scalability, and efficiency as compared with 10 existing popular BN learning algorithms. We apply SBN to a real-world application of brain connectivity modeling for Alzheimer's disease (AD) and reveal findings that could lead to advancements in AD research.},
  keywords = {to read},
  file = {/Users/pepijnvink/Zotero/storage/7U5ECPT4/Huang et al. - 2013 - A Sparse Structure Learning Algorithm for Gaussian.pdf}
}

@article{mokhtarian2023,
  title = {Novel {{Ordering-Based Approaches}} for {{Causal Structure Learning}} in the {{Presence}} of {{Unobserved Variables}}},
  author = {Mokhtarian, Ehsan and Khorasani, Mohmmadsadegh and Etesami, Jalal and Kiyavash, Negar},
  year = {2023},
  month = jun,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {37},
  number = {10},
  pages = {12260--12268},
  issn = {2374-3468},
  doi = {10.1609/aaai.v37i10.26445},
  urldate = {2023-11-22},
  abstract = {We propose ordering-based approaches for learning the maximal ancestral graph (MAG) of a structural equation model (SEM) up to its Markov equivalence class (MEC) in the presence of unobserved variables. Existing ordering-based methods in the literature recover a graph through learning a causal order (c-order). We advocate for a novel order called removable order (r-order) as they are advantageous over c-orders for structure learning. This is because r-orders are the minimizers of an appropriately defined optimization problem that could be either solved exactly (using a reinforcement learning approach) or approximately (using a hill-climbing search). Moreover, the r-orders (unlike c-orders) are invariant among all the graphs in a MEC and include c-orders as a subset. Given that set of r-orders is often significantly larger than the set of c-orders, it is easier for the optimization problem to find an r-order instead of a c-order. We evaluate the performance and the scalability of our proposed approaches on both real-world and randomly generated networks.},
  copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  keywords = {RU: Graphical Model,to read},
  file = {/Users/pepijnvink/Zotero/storage/THPCJ7YC/Mokhtarian et al. - 2023 - Novel Ordering-Based Approaches for Causal Structu.pdf}
}

@article{narisetty2014,
  title = {Bayesian Variable Selection with Shrinking and Diffusing Priors},
  author = {Narisetty, Naveen Naidu and He, Xuming},
  year = {2014},
  month = apr,
  journal = {The Annals of Statistics},
  volume = {42},
  number = {2},
  pages = {789--817},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/14-AOS1207},
  urldate = {2023-11-08},
  abstract = {We consider a Bayesian approach to variable selection in the presence of high dimensional covariates based on a hierarchical model that places prior distributions on the regression coefficients as well as on the model space. We adopt the well-known spike and slab Gaussian priors with a distinct feature, that is, the prior variances depend on the sample size through which appropriate shrinkage can be achieved. We show the strong selection consistency of the proposed method in the sense that the posterior probability of the true model converges to one even when the number of covariates grows nearly exponentially with the sample size. This is arguably the strongest selection consistency result that has been available in the Bayesian variable selection literature; yet the proposed method can be carried out through posterior sampling with a simple Gibbs sampler. Furthermore, we argue that the proposed method is asymptotically similar to model selection with the \$L\_\{0\}\$ penalty. We also demonstrate through empirical work the fine performance of the proposed approach relative to some state of the art alternatives.},
  keywords = {62F12,62F15,62J05,Bayes factor,hierarchical model,high dimensional data,read,shrinkage,Variable selection},
  file = {/Users/pepijnvink/Zotero/storage/WN7DWYWP/Narisetty and He - 2014 - Bayesian variable selection with shrinking and dif.pdf}
}

@article{ni2022,
  title = {Bayesian Graphical Models for Modern Biological Applications},
  author = {Ni, Yang and Baladandayuthapani, Veerabhadran and Vannucci, Marina and Stingo, Francesco C.},
  year = {2022},
  month = jun,
  journal = {Statistical Methods \& Applications},
  volume = {31},
  number = {2},
  pages = {197--225},
  issn = {1618-2510, 1613-981X},
  doi = {10.1007/s10260-021-00572-8},
  urldate = {2023-09-14},
  abstract = {Graphical models are powerful tools that are regularly used to investigate complex dependence structures in high-throughput biomedical datasets. They allow for holistic, systems-level view of the various biological processes, for intuitive and rigorous understanding and interpretations. In the context of large networks, Bayesian approaches are particularly suitable because it encourages sparsity of the graphs, incorporate prior information, and most importantly account for uncertainty in the graph structure. These features are particularly important in applications with limited sample size, including genomics and imaging studies. In this paper, we review several recently developed techniques for the analysis of large networks under non-standard settings, including but not limited to, multiple graphs for data observed from multiple related subgroups, graphical regression approaches used for the analysis of networks that change with covariates, and other complex sampling and structural settings. We also illustrate the practical utility of some of these methods using examples in cancer genomics and neuroimaging.},
  langid = {english},
  keywords = {read},
  file = {/Users/pepijnvink/Zotero/storage/LUHZMJHR/Ni et al. - 2022 - Bayesian graphical models for modern biological ap.pdf}
}

@article{roverato2004,
  title = {Compatible {{Prior Distributions}} for {{Directed Acyclic Graph Models}}},
  author = {Roverato, Alberto and Consonni, Guido},
  year = {2004},
  month = feb,
  journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume = {66},
  number = {1},
  pages = {47--61},
  issn = {1369-7412, 1467-9868},
  doi = {10.1111/j.1467-9868.2004.00431.x},
  urldate = {2023-09-14},
  abstract = {The application of certain Bayesian techniques, such as the Bayes factor and model averaging, requires the specification of prior distributions on the parameters of alternative models. We propose a new method for constructing compatible priors on the parameters of models nested in a given directed acyclic graph model, using a conditioning approach. We define a class of parameterizations that is consistent with the modular structure of the directed acyclic graph and derive a procedure, that is invariant within this class, which we name reference conditioning.},
  langid = {english},
  keywords = {to read},
  file = {/Users/pepijnvink/Zotero/storage/QKRSE5LC/Roverato and Consonni - 2004 - Compatible Prior Distributions for Directed Acycli.pdf}
}

@article{scanagatta2019,
  title = {A Survey on {{Bayesian}} Network Structure Learning from Data},
  author = {Scanagatta, Mauro and Salmer{\'o}n, Antonio and Stella, Fabio},
  year = {2019},
  month = dec,
  journal = {Progress in Artificial Intelligence},
  volume = {8},
  number = {4},
  pages = {425--439},
  issn = {2192-6360},
  doi = {10.1007/s13748-019-00194-y},
  urldate = {2023-09-14},
  abstract = {A necessary step in the development of artificial intelligence is to enable a machine to represent how the world works, building an internal structure from data. This structure should hold a good trade-off between expressive power and querying efficiency. Bayesian networks have proven to be an effective and versatile tool for the task at hand. They have been applied to modeling knowledge in a variety of fields, ranging from bioinformatics to law, from image processing to economic risk analysis. A crucial aspect is learning the dependency graph of a Bayesian network from data. This task, called structure learning, is NP-hard and is the subject of intense, cutting-edge research. In short, it can be thought of as choosing one graph over the many candidates, grounding our reasoning over a collection of samples of the distribution generating the data. The number of possible graphs increases very quickly at the increase in the number of variables. Searching in this space, and selecting a graph over the others, becomes quickly burdensome. In this survey, we review the most relevant structure learning algorithms that have been proposed in the literature. We classify them according to the approach they follow for solving the problem and we also show alternatives for handling missing data and continuous variable. An extensive review of existing software tools is also given.},
  langid = {english},
  keywords = {Bayesian network,Machine learning,Statistics,Structure learning,to read},
  file = {/Users/pepijnvink/Zotero/storage/XPXHY4GY/Scanagatta et al. - 2019 - A survey on Bayesian network structure learning fr.pdf}
}

@article{silva,
  title = {The {{Hidden Life}} of {{Latent Variables}}: {{Bayesian Learning}} with {{Mixed Graph Models}}},
  author = {Silva, Ricardo and Ghahramani, Zoubin},
  abstract = {Directed acyclic graphs (DAGs) have been widely used as a representation of conditional independence in machine learning and statistics. Moreover, hidden or latent variables are often an important component of graphical models. However, DAG models suffer from an important limitation: the family of DAGs is not closed under marginalization of hidden variables. This means that in general we cannot use a DAG to represent the independencies over a subset of variables in a larger DAG. Directed mixed graphs (DMGs) are a representation that includes DAGs as a special case, and overcomes this limitation. This paper introduces algorithms for performing Bayesian inference in Gaussian and probit DMG models. An important requirement for inference is the specification of the distribution over parameters of the models. We introduce a new distribution for covariance matrices of Gaussian DMGs. We discuss and illustrate how several Bayesian machine learning tasks can benefit from the principle presented here: the power to model dependencies that are generated from hidden variables, but without necessarily modeling such variables explicitly.},
  langid = {english},
  keywords = {to read},
  file = {/Users/pepijnvink/Zotero/storage/BGBCZNSX/Silva and Ghahramani - The Hidden Life of Latent Variables Bayesian Lear.pdf}
}

@article{spiegelhalter1990,
  title = {Sequential Updating of Conditional Probabilities on Directed Graphical Structures},
  author = {Spiegelhalter, David J. and Lauritzen, Steffen L.},
  year = {1990},
  month = aug,
  journal = {Networks},
  volume = {20},
  number = {5},
  pages = {579--605},
  issn = {0028-3045, 1097-0037},
  doi = {10.1002/net.3230200507},
  urldate = {2024-01-11},
  abstract = {Abstract             A directed acyclic graph or influence diagram is frequently used as a representation for qualitative knowledge in some domains in which expert system techniques have been applied, and conditional probability tables on appropriate sets of variables form the quantitative part of the accumulated experience. It is shown how one can introduce imprecision into such probabilities as a data base of cases accumulates. By exploiting the graphical structure, the updating can be performed locally, either approximately or exactly, and the setup makes it possible to take advantage of a range of well-established statistical techniques. As examples we discuss discrete models, models based on Dirichlet distributions and models of the logistic regression type.},
  langid = {english},
  file = {/Users/pepijnvink/Zotero/storage/YARM3JUN/Spiegelhalter and Lauritzen - 1990 - Sequential updating of conditional probabilities o.pdf}
}

@article{tsamardinos2006,
  title = {The Max-Min Hill-Climbing {{Bayesian}} Network Structure Learning Algorithm},
  author = {Tsamardinos, Ioannis and Brown, Laura E. and Aliferis, Constantin F.},
  year = {2006},
  month = oct,
  journal = {Machine Learning},
  volume = {65},
  number = {1},
  pages = {31--78},
  issn = {1573-0565},
  doi = {10.1007/s10994-006-6889-7},
  urldate = {2023-11-22},
  abstract = {We present a new algorithm for Bayesian network structure learning, called Max-Min Hill-Climbing (MMHC). The algorithm combines ideas from local learning, constraint-based, and search-and-score techniques in a principled and effective way. It first reconstructs the skeleton of a Bayesian network and then performs a Bayesian-scoring greedy hill-climbing search to orient the edges. In our extensive empirical evaluation MMHC outperforms on average and in terms of various metrics several prototypical and state-of-the-art algorithms, namely the PC, Sparse Candidate, Three Phase Dependency Analysis, Optimal Reinsertion, Greedy Equivalence Search, and Greedy Search. These are the first empirical results simultaneously comparing most of the major Bayesian network algorithms against each other. MMHC offers certain theoretical advantages, specifically over the Sparse Candidate algorithm, corroborated by our experiments. MMHC and detailed results of our study are publicly available at http://www.dsl-lab.org/supplements/mmhc\_paper/mmhc\_index.html.},
  langid = {english},
  keywords = {Bayesian networks,Graphical models,Structure learning,to read},
  file = {/Users/pepijnvink/Zotero/storage/74ZM4H5L/Tsamardinos et al. - 2006 - The max-min hill-climbing Bayesian network structu.pdf}
}

@misc{wang2014,
  title = {Learning Directed Acyclic Graphs via Bootstrap Aggregating},
  author = {Wang, Ru and Peng, Jie},
  year = {2014},
  month = jun,
  number = {arXiv:1406.2098},
  eprint = {1406.2098},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1406.2098},
  urldate = {2023-11-22},
  abstract = {Probabilistic graphical models are graphical representations of probability distributions. Graphical models have applications in many fields including biology, social sciences, linguistic, neuroscience. In this paper, we propose directed acyclic graphs (DAGs) learning via bootstrap aggregating. The proposed procedure is named as DAGBag. Specifically, an ensemble of DAGs is first learned based on bootstrap resamples of the data and then an aggregated DAG is derived by minimizing the overall distance to the entire ensemble. A family of metrics based on the structural hamming distance is defined for the space of DAGs (of a given node set) and is used for aggregation. Under the high-dimensional-low-sample size setting, the graph learned on one data set often has excessive number of false positive edges due to over-fitting of the noise. Aggregation overcomes over-fitting through variance reduction and thus greatly reduces false positives. We also develop an efficient implementation of the hill climbing search algorithm of DAG learning which makes the proposed method computationally competitive for the high-dimensional regime. The DAGBag procedure is implemented in the R package dagbag.},
  archiveprefix = {arxiv},
  keywords = {Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology,to read},
  file = {/Users/pepijnvink/Zotero/storage/EZTS23C8/Wang and Peng - 2014 - Learning directed acyclic graphs via bootstrap agg.pdf;/Users/pepijnvink/Zotero/storage/7B6LTEV9/1406.html}
}

@article{xie,
  title = {A {{Recursive Method}} for {{Structural Learning}} of {{Directed Acyclic Graphs}}},
  author = {Xie, Xianchao and Geng, Zhi},
  abstract = {In this paper, we propose a recursive method for structural learning of directed acyclic graphs (DAGs), in which a problem of structural learning for a large DAG is first decomposed into two problems of structural learning for two small vertex subsets, each of which is then decomposed recursively into two problems of smaller subsets until none subset can be decomposed further. In our approach, search for separators of a pair of variables in a large DAG is localized to small subsets, and thus the approach can improve the efficiency of searches and the power of statistical tests for structural learning. We show how the recent advances in the learning of undirected graphical models can be employed to facilitate the decomposition. Simulations are given to demonstrate the performance of the proposed method.},
  langid = {english},
  keywords = {to read},
  file = {/Users/pepijnvink/Zotero/storage/55HPHJ5R/Xie and Geng - A Recursive Method for Structural Learning of Dire.pdf}
}

@inproceedings{yu2021,
  title = {{{DAGs}} with {{No Curl}}: {{An Efficient DAG Structure Learning Approach}}},
  shorttitle = {{{DAGs}} with {{No Curl}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Yu, Yue and Gao, Tian and Yin, Naiyu and Ji, Qiang},
  year = {2021},
  month = jul,
  pages = {12156--12166},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-11-22},
  abstract = {Recently directed acyclic graph (DAG) structure learning is formulated as a constrained continuous optimization problem with continuous acyclicity constraints and was solved iteratively through subproblem optimization. To further improve efficiency, we propose a novel learning framework to model and learn the weighted adjacency matrices in the DAG space directly. Specifically, we first show that the set of weighted adjacency matrices of DAGs are equivalent to the set of weighted gradients of graph potential functions, and one may perform structure learning by searching in this equivalent set of DAGs. To instantiate this idea, we propose a new algorithm, DAG-NoCurl, which solves the optimization problem efficiently with a two-step procedure: \$1)\$ first we find an initial non-acyclic solution to the optimization problem, and \$2)\$ then we employ the Hodge decomposition of graphs and learn an acyclic graph by projecting the non-acyclic graph to the gradient of a potential function. Experimental studies on benchmark datasets demonstrate that our method provides comparable accuracy but better efficiency than baseline DAG structure learning methods on both linear and generalized structural equation models, often by more than one order of magnitude.},
  langid = {english},
  keywords = {to read},
  file = {/Users/pepijnvink/Zotero/storage/XRYRLZ9U/Yu et al. - 2021 - DAGs with No Curl An Efficient DAG Structure Lear.pdf}
}

@inproceedings{zhu2021,
  title = {Efficient and {{Scalable Structure Learning}} for {{Bayesian Networks}}: {{Algorithms}} and {{Applications}}},
  shorttitle = {Efficient and {{Scalable Structure Learning}} for {{Bayesian Networks}}},
  booktitle = {2021 {{IEEE}} 37th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  author = {Zhu, Rong and Pfadler, Andreas and Wu, Ziniu and Han, Yuxing and Yang, Xiaoke and Ye, Feng and Qian, Zhenping and Zhou, Jingren and Cui, Bin},
  year = {2021},
  month = apr,
  pages = {2613--2624},
  issn = {2375-026X},
  doi = {10.1109/ICDE51399.2021.00292},
  urldate = {2023-11-22},
  abstract = {Structure Learning for Bayesian network (BN) is an important problem with extensive research. It plays central roles in a wide variety of applications in Alibaba Group. However, existing structure learning algorithms suffer from considerable limitations in real-world applications due to their low efficiency and poor scalability. To resolve this, we propose a new structure learning algorithm LEAST, which comprehensively fulfills our business requirements as it attains high accuracy, efficiency and scalability at the same time. The core idea of LEAST is to formulate the structure learning into a continuous constrained optimization problem, with a novel differentiable constraint function measuring the acyclicity of the resulting graph. Unlike with existing work, our constraint function is built on the spectral radius of the graph and could be evaluated in near linear time w.r.t. the graph node size. Based on it, LEAST can be efficiently implemented with low storage overhead. According to our benchmark evaluation, LEAST runs 1{\textendash}2 orders of magnitude faster than state-of-the-art method with comparable accuracy, and it is able to scale on BNs with up to hundreds of thousands of variables. In our production environment, LEAST is deployed and serves for more than 20 applications with thousands of executions per day. We describe a concrete scenario in a ticket booking service in Alibaba, where LEAST is applied to build a near real-time automatic anomaly detection and root error cause analysis system. We also show that LEAST unlocks the possibility of applying BN structure learning in new areas, such as large-scale gene expression data analysis and explainable recommendation system.},
  keywords = {to read},
  file = {/Users/pepijnvink/Zotero/storage/8HB5YNFJ/Zhu et al. - 2021 - Efficient and Scalable Structure Learning for Baye.pdf}
}
